{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee6bb6f",
   "metadata": {},
   "source": [
    "# Stage 05 â€” Data Storage (Starter)\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Saving a DataFrame to **CSV** (`data/raw/`) and **Parquet** (`data/processed/`) using environment-driven paths.\n",
    "- Reloading and validating shape/dtypes.\n",
    "- Utility functions `write_df` / `read_df` routing by **file suffix**, with graceful handling when Parquet engine is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e658872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup: env paths & imports (English-only comments) ---\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env and resolve directories\n",
    "load_dotenv()\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR_RAW = Path(os.getenv(\"DATA_DIR_RAW\", \"data/raw\"))\n",
    "DATA_DIR_PROCESSED = Path(os.getenv(\"DATA_DIR_PROCESSED\", \"data/processed\"))\n",
    "DATA_DIR_RAW.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STAMP = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "print(\"RAW ->\", DATA_DIR_RAW.resolve())\n",
    "print(\"PROC->\", DATA_DIR_PROCESSED.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a small sample DataFrame (stable, no network) ---\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"date\": pd.date_range(end=pd.Timestamp.today().normalize(), periods=5, freq=\"D\"),\n",
    "    \"ticker\": [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\"],\n",
    "    \"close\": [189.2, 414.1, 168.3, 176.4, 512.7],\n",
    "    \"volume\": [1_234_000, 2_345_000, 1_111_000, 1_765_000, 2_222_000]\n",
    "})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27268b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Save in two formats (CSV in raw, Parquet in processed) ---\n",
    "csv_path = DATA_DIR_RAW / f\"sample_{STAMP}.csv\"\n",
    "pq_path = DATA_DIR_PROCESSED / f\"sample_{STAMP}.parquet\"\n",
    "\n",
    "# Save CSV\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Save Parquet (prefer pyarrow if available)\n",
    "engine = None\n",
    "try:\n",
    "    import pyarrow  # noqa: F401\n",
    "    engine = \"pyarrow\"\n",
    "except Exception:\n",
    "    engine = None\n",
    "\n",
    "if engine is None:\n",
    "    # Try to save with default (may fail if no engine installed)\n",
    "    try:\n",
    "        df.to_parquet(pq_path, index=False)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Parquet engine is missing. Please install pyarrow: `pip install pyarrow`.\"\n",
    "        ) from e\n",
    "else:\n",
    "    df.to_parquet(pq_path, index=False, engine=engine)\n",
    "\n",
    "print(\"Saved:\", csv_path)\n",
    "print(\"Saved:\", pq_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292618d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 2: Reload and validate ---\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "df_pq = pd.read_parquet(pq_path)\n",
    "\n",
    "print(\"Shapes:\", df_csv.shape, df_pq.shape)\n",
    "\n",
    "# Critical dtype expectations\n",
    "expected_dtypes = {\n",
    "    \"date\": \"datetime64[ns]\",\n",
    "    \"ticker\": \"object\",\n",
    "    \"close\": \"float64\",\n",
    "    \"volume\": \"int64\"\n",
    "}\n",
    "\n",
    "def coerce_and_check(d, expectations):\n",
    "    # Coerce\n",
    "    if \"date\" in d.columns:\n",
    "        d[\"date\"] = pd.to_datetime(d[\"date\"], errors=\"coerce\")\n",
    "    for c in [\"close\", \"volume\"]:\n",
    "        if c in d.columns:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "    # Report\n",
    "    ok_shape = (df_csv.shape == df_pq.shape)\n",
    "    print(\"Same shape:\", ok_shape)\n",
    "    problems = []\n",
    "    for col, exp in expectations.items():\n",
    "        if col not in d.columns:\n",
    "            problems.append(f\"{col}: missing\")\n",
    "            continue\n",
    "        actual = str(d[col].dtype)\n",
    "        if exp not in actual:\n",
    "            problems.append(f\"{col}: expected {exp}, got {actual}\")\n",
    "    return problems\n",
    "\n",
    "problems_csv = coerce_and_check(df_csv.copy(), expected_dtypes)\n",
    "problems_pq  = coerce_and_check(df_pq.copy(),  expected_dtypes)\n",
    "\n",
    "print(\"CSV dtype issues:\", problems_csv if problems_csv else \"None\")\n",
    "print(\"Parquet dtype issues:\", problems_pq if problems_pq else \"None\")\n",
    "\n",
    "df_csv.head(), df_pq.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 3: Utilities write_df / read_df ---\n",
    "from typing import Optional\n",
    "\n",
    "def write_df(df: pd.DataFrame, path: Path) -> None:\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix == \".csv\":\n",
    "        df.to_csv(path, index=False)\n",
    "    elif suffix == \".parquet\":\n",
    "        # Prefer pyarrow; provide a clear message if missing\n",
    "        try:\n",
    "            import pyarrow  # noqa: F401\n",
    "            df.to_parquet(path, index=False, engine=\"pyarrow\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Parquet engine missing. Install pyarrow: `pip install pyarrow`.\") from e\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported suffix: {suffix}\")\n",
    "\n",
    "def read_df(path: Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    elif suffix == \".parquet\":\n",
    "        try:\n",
    "            import pyarrow  # noqa: F401\n",
    "            return pd.read_parquet(path, engine=\"pyarrow\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Parquet engine missing. Install pyarrow: `pip install pyarrow`.\") from e\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported suffix: {suffix}\")\n",
    "\n",
    "# Demo the utilities\n",
    "csv2 = DATA_DIR_RAW / f\"sample_util_{STAMP}.csv\"\n",
    "pq2  = DATA_DIR_PROCESSED / f\"sample_util_{STAMP}.parquet\"\n",
    "write_df(df, csv2)\n",
    "write_df(df, pq2)\n",
    "r_csv = read_df(csv2)\n",
    "r_pq  = read_df(pq2)\n",
    "\n",
    "print(\"Utility read shapes:\", r_csv.shape, r_pq.shape)\n",
    "r_csv.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca160de",
   "metadata": {},
   "source": [
    "## Documentation (to include in README)\n",
    "\n",
    "- **Folder structure**\n",
    "  - `data/raw/`: immutable raw drops (CSV)\n",
    "  - `data/processed/`: typed, efficient Parquet used downstream\n",
    "- **Why CSV and Parquet**\n",
    "  - CSV is universal & human-readable; Parquet preserves dtypes and is smaller/faster for analytics.\n",
    "- **Env-driven IO**\n",
    "  - `.env` defines `DATA_DIR_RAW` and `DATA_DIR_PROCESSED`; code reads these and writes accordingly.\n",
    "- **Validation**\n",
    "  - Shapes must match between CSV and Parquet. Critical columns have expected dtypes (`date`, `ticker`, `close`, `volume`).\n",
    "\n",
    "You can copy this section into your project `README.md`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
