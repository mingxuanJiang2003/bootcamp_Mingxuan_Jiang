Risks for deployment of the current market‑direction classifier include schema drift as vendors change column names, silent data drift as volatility regimes shift, delayed or missing daily bars, label leakage from future data during retraining, and distribution shift between backtest and live when liquidity dries up. These failures would degrade calibration and drive false confidence in decisions.

Monitoring spans four layers. Data: schema hash on ingest, null rate by column, freshness (max minutes since last batch), and population stability index on key features (ret, vol, mom) with a 0.1 warning and 0.2 action threshold. Model: two‑week rolling AUC and Brier score, calibration slope/intercept weekly, and prediction drift (mean/variance). Trigger action if AUC < 0.62 for two consecutive weeks or Brier score worsens by 15% vs. baseline. System: API p95 latency, error rate, uptime, job success ratio for ETL and retraining pipelines, with alerts on latency > 300 ms, error rate > 1%, or any ETL job failure. Business: approval/usage rate of predictions in downstream rules, hit rate vs. benchmark, and PnL contribution; alert if weekly PnL contribution falls below the 20th percentile of backtest distribution.

Ownership: the platform on‑call owns system metrics and first response; the data engineering rota owns schema and freshness; the modeling owner maintains model metrics, defines thresholds, and runs rollbacks; the product analyst reviews business KPIs weekly. Alerts land in a shared channel with a runbook link. First step is to freeze promotion, switch to the previous model tag, and open an incident ticket; retraining is triggered by PSI > 0.2 or model underperformance for two weeks.