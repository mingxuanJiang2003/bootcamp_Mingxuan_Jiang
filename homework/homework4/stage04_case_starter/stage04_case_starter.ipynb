{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5222c733",
   "metadata": {},
   "source": [
    "# Stage 04 â€” Data Acquisition & Ingestion (Classroom Case, FIXED)\n",
    "\n",
    "This notebook includes the patched `validate_df` that avoids the `TypeError: arg must be a list`.\n",
    "Run cells from top to bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c27f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup & Utilities (English-only comments) ---\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Paths and timestamp\n",
    "ROOT = Path.cwd()\n",
    "DATA_RAW = ROOT / \"data\" / \"raw\"\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "STAMP = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "def _ensure_list(x):\n",
    "    \"\"\"Normalize a single string to a list, keep lists/tuples as-is, ignore None.\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return list(x)\n",
    "    return [x]\n",
    "\n",
    "def validate_df(\n",
    "    df: pd.DataFrame,\n",
    "    required_cols=None,\n",
    "    numeric_cols=None,\n",
    "    date_cols=None,\n",
    "    min_rows: int = 1\n",
    "):\n",
    "    \"\"\"Minimal validation for required columns, types, NA counts, and row count.\"\"\"\n",
    "    assert isinstance(df, pd.DataFrame), \"Input is not a DataFrame\"\n",
    "    assert len(df) >= min_rows, f\"Not enough rows: {len(df)}\"\n",
    "\n",
    "    required_cols = _ensure_list(required_cols)\n",
    "    numeric_cols  = _ensure_list(numeric_cols)\n",
    "    date_cols     = _ensure_list(date_cols)\n",
    "\n",
    "    if required_cols:\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        assert not missing, f\"Missing columns: {missing}\"\n",
    "\n",
    "    for c in date_cols:\n",
    "        if c in df.columns and not pd.api.types.is_datetime64_any_dtype(df[c]):\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=False)\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    na_counts = df.isna().sum()\n",
    "    if na_counts.any():\n",
    "        print(\"NA counts (non-zero only):\")\n",
    "        print(na_counts[na_counts > 0].sort_values(ascending=False))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715c22e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arg must be a list, tuple, 1-d array, or Series",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m api_df = \u001b[43mfetch_market_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTICKER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m api_df.head()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mfetch_market_data\u001b[39m\u001b[34m(ticker)\u001b[39m\n\u001b[32m     17\u001b[39m cols = [\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mopen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33madjusted_close\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvolume\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     18\u001b[39m df = df[cols]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m df = \u001b[43mvalidate_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequired_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopen\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhigh\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclose\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvolume\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopen\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhigh\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclose\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madjusted_close\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvolume\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.sort_values(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m outpath = DATA_RAW / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mapi_yfinance_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTAMP\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m df.to_csv(outpath, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mvalidate_df\u001b[39m\u001b[34m(df, required_cols, numeric_cols, date_cols, min_rows)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m numeric_cols:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         df[c] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoerce\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m na_counts = df.isna().sum()\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_counts.any():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\24614\\bootcamp_Mingxuan_Jiang\\homework\\.venv\\Lib\\site-packages\\pandas\\core\\tools\\numeric.py:209\u001b[39m, in \u001b[36mto_numeric\u001b[39m\u001b[34m(arg, errors, downcast, dtype_backend)\u001b[39m\n\u001b[32m    207\u001b[39m     values = np.array([arg], dtype=\u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33marg must be a list, tuple, 1-d array, or Series\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    211\u001b[39m     values = arg\n",
      "\u001b[31mTypeError\u001b[39m: arg must be a list, tuple, 1-d array, or Series"
     ]
    }
   ],
   "source": [
    "# --- Part A: API-like fetch via yfinance (no key required) ---\n",
    "import yfinance as yf\n",
    "\n",
    "TICKER = \"AAPL\"  # You may change this to any valid ticker\n",
    "\n",
    "def fetch_market_data(ticker: str) -> pd.DataFrame:\n",
    "    # Download last 6 months of daily data\n",
    "    df = yf.download(ticker, period=\"6mo\", interval=\"1d\", auto_adjust=False, progress=False)\n",
    "    df = df.reset_index().rename(columns=str.lower)  # 'Date' -> 'date', etc.\n",
    "    # Ensure adjusted_close exists even if not provided\n",
    "    if \"adj close\" in df.columns:\n",
    "        df = df.rename(columns={\"adj close\": \"adjusted_close\"})\n",
    "    elif \"adjusted_close\" not in df.columns:\n",
    "        import numpy as np\n",
    "        df[\"adjusted_close\"] = np.nan\n",
    "\n",
    "    cols = [\"date\", \"open\", \"high\", \"low\", \"close\", \"adjusted_close\", \"volume\"]\n",
    "    df = df[cols]\n",
    "\n",
    "    df = validate_df(\n",
    "        df,\n",
    "        required_cols=[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"],\n",
    "        numeric_cols=[\"open\", \"high\", \"low\", \"close\", \"adjusted_close\", \"volume\"],\n",
    "        date_cols=[\"date\"],\n",
    "        min_rows=5\n",
    "    ).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    outpath = DATA_RAW / f\"api_yfinance_{ticker.upper()}_{STAMP}.csv\"\n",
    "    df.to_csv(outpath, index=False)\n",
    "    print(f\"Saved: {outpath}\")\n",
    "    return df\n",
    "\n",
    "api_df = fetch_market_data(TICKER)\n",
    "api_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6397aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part B: Scrape a small public table (Wikipedia: DJIA constituents) ---\n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average\"\n",
    "\n",
    "def fetch_djia_table(url: str = WIKI_URL) -> pd.DataFrame:\n",
    "    html = requests.get(url, timeout=30).text\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    candidate = None\n",
    "    for tbl in soup.select(\"table.wikitable\"):\n",
    "        headers = [th.get_text(strip=True).lower() for th in tbl.select(\"tr th\")]\n",
    "        if any(\"symbol\" in h for h in headers) and any((\"company\" in h) or (\"constituent\" in h) or (\"name\" in h) for h in headers):\n",
    "            candidate = tbl\n",
    "            break\n",
    "    if candidate is None:\n",
    "        candidate = soup.select_one(\"table.wikitable\")\n",
    "\n",
    "    rows = []\n",
    "    headers = [th.get_text(strip=True) for th in candidate.select(\"tr th\")]\n",
    "    for tr in candidate.select(\"tr\")[1:]:\n",
    "        cells = [td.get_text(strip=True) for td in tr.select(\"td\")]\n",
    "        if not cells or len(cells) < 2:\n",
    "            continue\n",
    "        row = {}\n",
    "        for i, val in enumerate(cells):\n",
    "            col = headers[i] if i < len(headers) else f\"col_{i}\"\n",
    "            row[col] = val\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    rename_map = {}\n",
    "    for col in df.columns:\n",
    "        low = col.lower()\n",
    "        if \"symbol\" in low: rename_map[col] = \"Symbol\"\n",
    "        if (\"company\" in low) or (\"constituent\" in low) or (\"name\" in low): rename_map[col] = \"Company\"\n",
    "        if \"weight\" in low: rename_map[col] = \"Weight\"\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    keep = [c for c in [\"Symbol\", \"Company\", \"Weight\"] if c in df.columns]\n",
    "    if not keep:\n",
    "        keep = df.columns[:3]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    numeric_cols = [c for c in df.columns if c.lower() in {\"weight\"}]\n",
    "    df = validate_df(df, required_cols=[keep[0], keep[1]], numeric_cols=numeric_cols, min_rows=10)\n",
    "\n",
    "    outpath = DATA_RAW / f\"scrape_wikipedia_djia_{STAMP}.csv\"\n",
    "    df.to_csv(outpath, index=False)\n",
    "    print(f\"Saved: {outpath}\")\n",
    "    return df\n",
    "\n",
    "scrape_df = fetch_djia_table()\n",
    "scrape_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4edb17",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "**Sources**\n",
    "- API-like source: `yfinance` for daily OHLCV data. No API key required.\n",
    "- Scraped table: Wikipedia page \"Dow Jones Industrial Average\".\n",
    "\n",
    "**Parameters**\n",
    "- Market data: `period=\"6mo\"`, `interval=\"1d\"`, `auto_adjust=False`.\n",
    "- Scraping: first `wikitable` that contains headers similar to \"Symbol\" and \"Company\".\n",
    "\n",
    "**Validation**\n",
    "- Market data must contain `date, open, high, low, close, volume`.\n",
    "- Convert date to datetime and price/volume to numeric.\n",
    "- Minimum row count: 5.\n",
    "- Scraped table must contain at least two descriptive columns (`Symbol`, `Company`) and at least 10 rows.\n",
    "\n",
    "**File Naming & Location**\n",
    "- Save raw CSVs under `data/raw/` with a timestamp.\n",
    "  - `api_yfinance_<TICKER>_<YYYYMMDD-HHMM>.csv`\n",
    "  - `scrape_wikipedia_djia_<YYYYMMDD-HHMM>.csv`\n",
    "\n",
    "**Assumptions & Risks**\n",
    "- Wikipedia layout may change; parsing uses flexible selectors.\n",
    "- Free data sources can have occasional delays or missing fields.\n",
    "- Market holidays can create gaps in dates.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
